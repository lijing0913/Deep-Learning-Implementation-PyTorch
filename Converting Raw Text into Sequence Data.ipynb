{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c8d942",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# Converting Raw Text into Sequence Data\n",
    "\n",
    "Typical preprocessing pipelines\n",
    "execute the following steps:\n",
    "\n",
    "1. Load text as strings into memory.\n",
    "1. Split the strings into tokens (e.g., words or characters).\n",
    "1. Build a vocabulary dictionary to associate each vocabulary element with a numerical index.\n",
    "1. Convert the text into sequences of numerical indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f51b57",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T05:05:56.284016Z",
     "iopub.status.busy": "2022-12-14T05:05:56.283510Z",
     "iopub.status.idle": "2022-12-14T05:05:58.619010Z",
     "shell.execute_reply": "2022-12-14T05:05:58.618107Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dc07a",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "Here, we will work with H. G. Wells'\n",
    "[The Time Machine](http://www.gutenberg.org/ebooks/35),\n",
    "a book containing just over 30000 words.\n",
    "While real applications will typically\n",
    "involve significantly larger datasets,\n",
    "this is sufficient to demonstrate\n",
    "the preprocessing pipeline.\n",
    "The following `_download` method\n",
    "**reads the raw text into a string**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba10dc0",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T05:05:58.623593Z",
     "iopub.status.busy": "2022-12-14T05:05:58.622595Z",
     "iopub.status.idle": "2022-12-14T05:05:58.760495Z",
     "shell.execute_reply": "2022-12-14T05:05:58.759474Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n",
      "# Total line of texts: 3221\n",
      "the time machine by h g wells\n",
      "twinkled and his usually pale face was flushed and animated the\n"
     ]
    }
   ],
   "source": [
    "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n",
    "                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
    "\n",
    "def read_time_machine():  \n",
    "    \"\"\"Load raw text to string list\"\"\"\n",
    "    with open(d2l.download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines] # convert punctuation and capitalization to space\n",
    "\n",
    "lines = read_time_machine()\n",
    "print(f'# Total line of texts: {len(lines)}')\n",
    "print(lines[0])\n",
    "print(lines[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6127a14c",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "For simplicity, we ignore punctuation and capitalization when preprocessing the raw text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47758c",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "*Tokens* are the atomic (indivisible) units of text.\n",
    "Each time step corresponds to 1 token,\n",
    "but what precisely constitutes a token is a design choice.\n",
    "For example, we could represent the sentence\n",
    "\"Baby needs a new pair of shoes\"\n",
    "as a sequence of 7 words,\n",
    "where the set of all words comprise\n",
    "a large vocabulary (typically tens\n",
    "or hundreds of thousands of words).\n",
    "Or we would represent the same sentence\n",
    "as a much longer sequence of 30 characters,\n",
    "using a much smaller vocabulary\n",
    "(there are only 256 distinct ASCII characters).\n",
    "Below, we tokenize our preprocessed text\n",
    "into a sequence of characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26417492",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T05:05:58.787383Z",
     "iopub.status.busy": "2022-12-14T05:05:58.786856Z",
     "iopub.status.idle": "2022-12-14T05:05:58.794716Z",
     "shell.execute_reply": "2022-12-14T05:05:58.793778Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['i']\n",
      "[]\n",
      "[]\n",
      "['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']\n",
      "['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\n",
      "['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(lines, token='word'):  \n",
    "    \"\"\"Tokenize the raw text to word or char\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('Error: Unknown token type: ' + token)\n",
    "\n",
    "tokens = tokenize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ffb99",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "## Vocabulary\n",
    "\n",
    "These tokens are still strings.\n",
    "However, the inputs to our models\n",
    "must ultimately consist\n",
    "of numerical inputs.\n",
    "**Next, we introduce a class\n",
    "for constructing *vocabularies*,\n",
    "i.e., objects that associate\n",
    "each distinct token value\n",
    "with a unique index.**\n",
    "First, we determine the set of unique tokens in our training *corpus*.\n",
    "We then assign a numerical index to each unique token.\n",
    "Rare vocabulary elements are often dropped for convenience.\n",
    "Whenever we encounter a token at training or test time\n",
    "that had not been previously seen or was dropped from the vocabulary,\n",
    "we represent it by a special \"&lt;unk&gt;\" token,\n",
    "signifying that this is an *unknown* value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc4475c2",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T05:05:58.798401Z",
     "iopub.status.busy": "2022-12-14T05:05:58.797759Z",
     "iopub.status.idle": "2022-12-14T05:05:58.806900Z",
     "shell.execute_reply": "2022-12-14T05:05:58.806093Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:  \n",
    "    \"\"\"Vocabulary for text.\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None): # min_freq: if frequency < min_freq, assign as unk\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # Sort based on token frequencies\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "        # index for the unknown token is 0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # Index for the unknown token\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  \n",
    "    \"\"\"Count token frequencies\"\"\"\n",
    "    # here tokens are 1D list or 2D list (list of list)\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # convert token list to a list\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7680e",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "We now **construct a vocabulary** for our dataset,\n",
    "converting the sequence of strings\n",
    "into a list of numerical indices.\n",
    "Note that we have not lost any information\n",
    "and can easily convert our dataset\n",
    "back to its original (string) representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b970d089",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T05:05:58.810582Z",
     "iopub.status.busy": "2022-12-14T05:05:58.809960Z",
     "iopub.status.idle": "2022-12-14T05:05:58.824662Z",
     "shell.execute_reply": "2022-12-14T05:05:58.823711Z"
    },
    "origin_pos": 15,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<unk>', 0), ('the', 1), ('i', 2), ('and', 3), ('of', 4), ('a', 5), ('to', 6), ('was', 7), ('in', 8), ('that', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f66b9c",
   "metadata": {},
   "source": [
    "Convert each line of text to a list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21500345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: ['the', 'time', 'machine', 'by', 'h', 'g', 'wells']\n",
      "indices: [1, 19, 50, 40, 2183, 2184, 400]\n",
      "words: ['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']\n",
      "indices: [2186, 3, 25, 1044, 362, 113, 7, 1421, 3, 1045, 1]\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10]:\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7b886",
   "metadata": {
    "origin_pos": 16
   },
   "source": [
    "## Putting It All Together\n",
    "\n",
    "Using the above classes and methods,\n",
    "we **package everything into the following\n",
    "`load_corpus_time_machine` function**,\n",
    "which returns `corpus`, a list of token indices, and `vocab`,\n",
    "the vocabulary of *The Time Machine* corpus.\n",
    "The modifications we did here are:\n",
    "(i) we tokenize text into characters, not words,\n",
    "to simplify the training in later sections;\n",
    "(ii) `corpus` is a single list, not a list of token lists,\n",
    "since each text line in *The Time Machine* dataset\n",
    "is not necessarily a sentence or paragraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71f6477",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    },
    "execution": {
     "iopub.execute_input": "2022-12-14T05:05:58.827913Z",
     "iopub.status.busy": "2022-12-14T05:05:58.827654Z",
     "iopub.status.idle": "2022-12-14T05:05:59.010539Z",
     "shell.execute_reply": "2022-12-14T05:05:59.009752Z"
    },
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170580, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_corpus_time_machine(max_tokens=-1):  \n",
    "    \"\"\"Return a list of token indices and the vocabulary of The Time Machine corpus\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00e5bb",
   "metadata": {},
   "source": [
    "There are 28 kinds of tokens in total, i.e., 26 characters, space, and the <unk> token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d359ff",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## Summary\n",
    "\n",
    "Text is among the most common forms of sequence data encountered in deep learning.\n",
    "Common choices for what constitutes a token are characters, words, and word pieces.\n",
    "To preprocess text, we usually (i) split text into tokens; (ii) build a vocabulary to map token strings to numerical indices; and (iii) convert text data into token indices for models to manipulate.\n",
    "In practice, the frequency of words tends to follow Zipf's law. This is true not just for individual words (unigrams), but also for $n$-grams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
