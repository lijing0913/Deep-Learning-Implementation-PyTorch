{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1380c5",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "### Architecture\n",
    "\n",
    "In AlexNet's first layer, the convolution window shape is $11\\times11$.\n",
    "Since the images in ImageNet are eight times higher and wider\n",
    "than the MNIST images,\n",
    "objects in ImageNet data tend to occupy more pixels with more visual detail.\n",
    "Consequently, a larger convolution window is needed to capture the object.\n",
    "The convolution window shape in the second layer\n",
    "is reduced to $5\\times5$, followed by $3\\times3$.\n",
    "In addition, after the first, second, and fifth convolutional layers,\n",
    "the network adds max-pooling layers\n",
    "with a window shape of $3\\times3$ and a stride of 2.\n",
    "Moreover, AlexNet has ten times more convolution channels than LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0198fb28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T05:41:35.617912Z",
     "iopub.status.busy": "2022-12-14T05:41:35.617636Z",
     "iopub.status.idle": "2022-12-14T05:41:37.876131Z",
     "shell.execute_reply": "2022-12-14T05:41:37.875242Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8611c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2cecc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (input_channel, output_channel, kernel_size, stride, padding)\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), # NOTE: here channel=1 because will test on Fashion-MNIST\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),\n",
    "    nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), # here 6400 = 256 * 5 * 5\n",
    "    nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 10) # for Fashion-MNIST, 10 categories\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61c156eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten Output shape:\t torch.Size([1, 6400])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'Output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088dfcd",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Test on the Fashion-MNIST dataset.\n",
    "We **construct a single-channel data example** with both height and width of 224 (**to observe the output shape of each layer**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ffe3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs = 0.01, 10\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb79c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
