{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1380c5",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Multi-Branch Networks  (GoogLeNet)\n",
    "\n",
    "## Inception Blocks\n",
    "\n",
    "The basic convolutional block in GoogLeNet is called an *Inception block*,\n",
    "stemming from the meme \"we need to go deeper\" of the movie *Inception*.\n",
    "\n",
    "The inception block consists of four parallel branches.\n",
    "The first three branches use convolutional layers\n",
    "with window sizes of $1\\times 1$, $3\\times 3$, and $5\\times 5$\n",
    "to extract information from different spatial sizes.\n",
    "The middle two branches also add a $1\\times 1$ convolution of the input\n",
    "to reduce the number of channels, reducing the model's complexity.\n",
    "The fourth branch uses a $3\\times 3$ max-pooling layer,\n",
    "followed by a $1\\times 1$ convolutional layer\n",
    "to change the number of channels.\n",
    "The four branches all use appropriate padding to give the input and output the same height and width.\n",
    "Finally, the outputs along each branch are concatenated\n",
    "along the channel dimension and comprise the block's output.\n",
    "The commonly-tuned hyperparameters of the Inception block\n",
    "are the number of output channels per layer, i.e., how to allocate capacity among convolutions of different size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0198fb28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T05:41:35.617912Z",
     "iopub.status.busy": "2022-12-14T05:41:35.617636Z",
     "iopub.status.idle": "2022-12-14T05:41:37.876131Z",
     "shell.execute_reply": "2022-12-14T05:41:37.875242Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8611c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c2cecc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    # c1--c4: the number of output channel for each path\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # path 1: 1x1 convolutional layer\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        # path 2: 1x1 convolutional layer + 3x3 convolutional layer\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        # path 3: 1x1 convolutional layer + 5x5 convolutional layer\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        # path 4: 3x3 max pooling layer + 1x1 convolutional layer\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # concatenate along output_channel dimension\n",
    "        return torch.cat((p1, p2, p3, p4), dim=1) # (batch_size, out_channels, ...), axis=1 refers to the dimension of out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97a6d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 stages\n",
    "\n",
    "b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d47bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "                   nn.ReLU(),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "541de8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "                   Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e84f96ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "                   Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "                   Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "                   Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "                   Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da5c9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "                   Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "                   nn.AdaptiveAvgPool2d((1,1)),\n",
    "                   nn.Flatten())\n",
    "\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61c156eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 1024])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(size=(1, 1, 96, 96))\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088dfcd",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Test on the Fashion-MNIST dataset.\n",
    "We **construct a single-channel data example** with both height and width of 224 (**to observe the output shape of each layer**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ffe3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
