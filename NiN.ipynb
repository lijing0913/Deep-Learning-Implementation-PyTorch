{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e1380c5",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Network in Network (NiN)\n",
    ":label:`sec_nin`\n",
    "\n",
    "LeNet, AlexNet, and VGG all share a common design pattern:\n",
    "extract features exploiting *spatial* structure\n",
    "via a sequence of convolutions and pooling layers\n",
    "and post-process the representations via fully connected layers.\n",
    "The improvements upon LeNet by AlexNet and VGG mainly lie\n",
    "in how these later networks widen and deepen these two modules.\n",
    "\n",
    "This design poses two major challenges.\n",
    "First, the fully connected layers at the end\n",
    "of the architecture consume tremendous numbers of parameters. \n",
    "\n",
    "Second, it is equally impossible to add fully connected layers\n",
    "earlier in the network to increase the degree of nonlinearity: doing so would destroy the\n",
    "spatial structure and require potentially even more memory.\n",
    "\n",
    "The *network in network* (*NiN*) blocks offer an alternative,\n",
    "capable of solving both problems in one simple strategy.\n",
    "They were proposed based on a very simple insight: (i) use $1 \\times 1$ convolutions to add\n",
    "local nonlinearities across the channel activations and (ii) use global average pooling to integrate\n",
    "across all locations in the last representation layer. Note that global average pooling would not\n",
    "be effective, were it not for the added nonlinearities. \n",
    "\n",
    "\n",
    "## NiN Blocks\n",
    "\n",
    "The inputs and outputs of convolutional layers\n",
    "consist of four-dimensional tensors with axes\n",
    "corresponding to the example, channel, height, and width.\n",
    "Also recall that the inputs and outputs of fully connected layers\n",
    "are typically two-dimensional tensors corresponding to the example and feature.\n",
    "The idea behind NiN is to apply a fully connected layer\n",
    "at each pixel location (for each height and width).\n",
    "**The resulting $1 \\times 1$ convolution can be thought as\n",
    "a fully connected layer acting independently on each pixel location**.\n",
    "\n",
    "Note both the difference in the NiN blocks (the initial convolution is followed by $1 \\times 1$ convolutions, whereas VGG retains $3 \\times 3$ convolutions) and in the end where we no longer require a giant fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0198fb28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T05:41:35.617912Z",
     "iopub.status.busy": "2022-12-14T05:41:35.617636Z",
     "iopub.status.idle": "2022-12-14T05:41:37.876131Z",
     "shell.execute_reply": "2022-12-14T05:41:37.875242Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c2cecc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), \n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), \n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05fce7",
   "metadata": {},
   "source": [
    "## NiN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6da86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nin_block(1, 96, kernel_size=11, strides=4, padding=0), # in_channel=1\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2d(3, stride=2), nn.Dropout(p=0.5),\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1), # 10 categories\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten()) # (batch_size, output_channel=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c156eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout Output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential Output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d Output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten Output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'Output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088dfcd",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Test on the Fashion-MNIST dataset.\n",
    "We **construct a single-channel data example** with both height and width of 224 (**to observe the output shape of each layer**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ffe3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 0.1, 10, 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
